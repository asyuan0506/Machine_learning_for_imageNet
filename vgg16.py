import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import models, datasets, transforms
from torch.utils.data import DataLoader
from tqdm import tqdm
import matplotlib.pyplot as plt
import matplotlib
import os

'''
è¨“ç·´ VGG16 æ¨¡å‹ï¼ˆä¸ä½¿ç”¨é è¨“ç·´æ¬Šé‡ï¼‰
ä¸¦ä¸”ä½¿ç”¨æ—©æœŸåœæ­¢ï¼ˆEarly Stopping)
è¨“ç·´éç¨‹ä¸­æœƒå„²å­˜æœ€ä½³æ¨¡å‹ï¼ˆé©—è­‰æº–ç¢ºç‡æœ€é«˜çš„æ¨¡å‹ï¼‰
è¨“ç·´çµæœæœƒç¹ªè£½ Loss å’Œ Accuracy æ›²ç·šåœ–
'''

# è¨­å®šä¸­æ–‡å­—å‹
matplotlib.rcParams['font.family'] = 'Microsoft JhengHei'
matplotlib.rcParams['axes.unicode_minus'] = False

def main():
    train_dir = "../ImageNet/train_t12_ready_100"
    val_dir = "../ImageNet/val_t12_ready_100"
    batch_size = 16
    num_classes = 100 # é¡åˆ¥æ•¸é‡
    num_epochs = 100 # è¨“ç·´çš„ epoch æ•¸é‡
    patience = 10 # æå‰åœæ­¢çš„è€å¿ƒæ¬¡æ•¸(early stopping patience)
    delta = 0.01  # é©—è­‰æº–ç¢ºç‡çš„æœ€å°æå‡é‡

    # é è™•ç†ï¼ˆresize + normalizationï¼‰
    train_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.RandomHorizontalFlip(), # éš¨æ©Ÿæ°´å¹³ç¿»è½‰
        transforms.RandomVerticalFlip(), # éš¨æ©Ÿå‚ç›´ç¿»è½‰
        transforms.RandomRotation(10), # éš¨æ©Ÿæ—‹è½‰
        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1), # éš¨æ©Ÿé¡è‰²è®ŠåŒ–(æ¨¡æ“¬å…‰ç·šä¸åŒ)
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                             std=[0.229, 0.224, 0.225])
    ])

    val_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                             std=[0.229, 0.224, 0.225])
    ])

    # å»ºç«‹è³‡æ–™é›†
    train_dataset = datasets.ImageFolder(train_dir, transform=train_transform)
    val_dataset = datasets.ImageFolder(val_dir, transform=val_transform)
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)

    # å»ºç«‹æ¨¡å‹
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = models.vgg16(weights=None)
    model.classifier[6] = nn.Linear(4096, num_classes)  # æ›¿æ›åˆ†é¡å±¤
    model = model.to(device)

    # è¨“ç·´åƒæ•¸
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=1e-4)

    train_losses, val_losses = [], []
    train_accuracies, val_accuracies = [], []

    best_val_acc = 0.0
    best_epoch = 0
    early_stop_counter = 0
    os.makedirs("checkpoints", exist_ok=True)

    # è¨“ç·´è¿´åœˆ
    for epoch in range(num_epochs):
        running_loss = 0.0
        train_correct = 0
        train_total = 0
        for images, labels in tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs} - Training"):
            images, labels = images.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            _, predicted = outputs.max(1)
            train_correct += (predicted == labels).sum().item()
            train_total += labels.size(0)

        train_loss = running_loss / len(train_loader)
        train_acc = train_correct / train_total * 100
        train_losses.append(train_loss)
        train_accuracies.append(train_acc)
        print(f"[Epoch {epoch+1}] Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%")

        # é©—è­‰
        model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0
        with torch.no_grad():
            for images, labels in val_loader:
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                loss = criterion(outputs, labels)
                val_loss += loss.item()
                _, predicted = outputs.max(1)
                val_correct += (predicted == labels).sum().item()
                val_total += labels.size(0)

        val_loss /= len(val_loader)
        val_acc = val_correct / val_total * 100
        val_losses.append(val_loss)
        val_accuracies.append(val_acc)
        print(f"[Epoch {epoch+1}] Validation Loss: {val_loss:.4f}, Validatoin Accuracy: {val_acc:.2f}%")

        # å„²å­˜æœ€ä½³æ¨¡å‹
        if val_acc > best_val_acc + delta:
            best_val_acc = val_acc
            best_epoch = epoch + 1
            early_stop_counter = 0
            save_path = f"checkpoints/vgg16_best_epoch{best_epoch:02d}_acc{val_acc:.2f}.pth"
            torch.save(model.state_dict(), save_path)
            print(f"ğŸ“¦ å·²å„²å­˜æœ€ä½³æ¨¡å‹ï¼š{save_path}")
        else:
            early_stop_counter += 1
            print(f"â¸ï¸ é©—è­‰æº–ç¢ºç‡ç„¡æ˜é¡¯é€²æ­¥ ({early_stop_counter}/{patience})")
            if early_stop_counter >= patience:
                print(f"ğŸ›‘ æå‰åœæ­¢è¨“ç·´æ–¼ Epoch {epoch+1}ï¼ˆæœ€ä½³ç‚º Epoch {best_epoch}ï¼‰")
                break

    # ç¹ªè£½è¨“ç·´çµæœåœ–
    epochs = list(range(1, num_epochs+1))
    plt.subplot(1, 2, 1)
    plt.plot(epochs, train_losses, label='è¨“ç·´ Loss')
    plt.plot(epochs, val_losses, label='é©—è­‰ Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Loss æ›²ç·š')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(epochs, train_accuracies, label='è¨“ç·´æº–ç¢ºç‡')
    plt.plot(epochs, val_accuracies, label='é©—è­‰æº–ç¢ºç‡')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy (%)')
    plt.title('æº–ç¢ºç‡æ›²ç·š')
    plt.legend()

    plt.tight_layout()
    plt.savefig("training_result_plot.png")
    print("ğŸ“ˆ è¨“ç·´çµæœå·²å„²å­˜ç‚º training_result_plot.png")

if __name__ == '__main__':
    main()
